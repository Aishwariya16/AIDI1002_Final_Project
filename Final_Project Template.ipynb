{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tr_jEBnh-jv"
   },
   "source": [
    "# Title: AIDI FINAL PROJECT\n",
    "\n",
    "#### Group Member Names : Aishwariya Balaji, Pardeep Kaur\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeKSxMvrh-j0"
   },
   "source": [
    "### INTRODUCTION:\n",
    "Text classification is a fundamental task in natural language processing (NLP) with applications in news categorization, sentiment analysis, spam detection, and more. This project focuses on implementing and evaluating a Lightweight TextCNN model for text classification, inspired by the architecture proposed in Light-Weighted CNN for Text Classification\n",
    "*********************************************************************************************************************\n",
    "#### AIM :\n",
    "The primary goal is twofold:\n",
    "\n",
    "Reproduction — faithfully replicate the performance of the proposed Lightweight TextCNN model using the AG_NEWS dataset.\n",
    "\n",
    "Contribution — extend the original work by introducing a baseline MLP model for comparison and experimenting with a dual-optimizer training strategy (Adam followed by SGD after validation plateau) to improve generalization.\n",
    "\n",
    "The AG_NEWS dataset contains news articles labeled into four categories: World, Sports, Business, and Sci/Tech. The models are implemented in PyTorch, with preprocessing handled through the HuggingFace datasets library for efficiency and reproducibility.\n",
    "\n",
    "By comparing the Lightweight TextCNN to the MLP baseline, this project aims to highlight the advantages of convolutional architectures for capturing local n-gram features in text data, while also exploring training techniques that can improve final performance.\n",
    "*********************************************************************************************************************\n",
    "#### Github Repo:\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### DESCRIPTION OF PAPER:\n",
    "The paper introduces a compact CNN architecture for text classification that reduces computational cost while maintaining high accuracy. It replaces standard convolutions with depthwise separable convolutions, drastically lowering parameter counts. Multiple kernel sizes capture diverse n-gram features, and global max pooling distills the most important signals. Experiments show the model matches or outperforms traditional CNNs on benchmark datasets with significantly fewer resources, making it ideal for deployment in low-resource environments.\n",
    "*********************************************************************************************************************\n",
    "#### PROBLEM STATEMENT :\n",
    "Text classification models often achieve high accuracy but at the cost of large model sizes and high computational requirements, making them impractical for deployment in resource-constrained environments. The challenge is to design a model that maintains strong classification performance while reducing computational complexity and parameter count. This project addresses the problem by reproducing and extending the Lightweight TextCNN architecture proposed by Yadav (2020) and comparing it against a baseline MLP model on the AG_NEWS dataset.\n",
    "*********************************************************************************************************************\n",
    "#### CONTEXT OF THE PROBLEM:\n",
    "With the exponential growth of textual data from news articles, social media, and online platforms, text classification has become a crucial task in natural language processing. While deep learning models such as CNNs and Transformers deliver high accuracy, they often require significant computational power and memory, which limits their use on mobile devices, embedded systems, or real-time applications. This creates a need for lightweight yet accurate architectures that can operate efficiently without sacrificing performance. The Lightweight TextCNN proposed by Yadav (2020) addresses this need by using depthwise separable convolutions to reduce complexity, making it an ideal candidate for scenarios where both speed and accuracy are essential.\n",
    "*********************************************************************************************************************\n",
    "#### SOLUTION:\n",
    "To address the need for efficient yet accurate text classification, this project implements and evaluates the Lightweight TextCNN architecture proposed by Yadav (2020). The model uses depthwise separable convolutions with multiple kernel sizes to capture diverse n-gram features while drastically reducing parameter count and computational cost.\n",
    "\n",
    "The solution involves:\n",
    "\n",
    "Reproducing the Lightweight TextCNN using the AG_NEWS dataset via the HuggingFace datasets library.\n",
    "\n",
    "Implementing a baseline MLP model for performance comparison.\n",
    "\n",
    "Introducing a dual-optimizer training strategy—starting with Adam for fast convergence, then switching to SGD when validation performance plateaus to improve generalization.\n",
    "\n",
    "Evaluating both models on accuracy, macro-F1 score, and parameter efficiency.\n",
    "\n",
    "This approach ensures a balance between high predictive performance and low resource usage, making the model practical for deployment in constrained environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77PIPLQ-h-j1"
   },
   "source": [
    "# Background\n",
    "*********************************************************************************************************************\n",
    "\n",
    "\n",
    "|Reference|Explanation|Dataset/Input|Weakness|\n",
    "|------|------|------|------|\n",
    "\n",
    "\n",
    "Reference\tExplanation\tDataset/Input\tWeakness\n",
    "Kaggle Customer Shopping Dataset\tThe dataset captures customer shopping behavior, including transactional, demographic, and product-level information. It is commonly used for sales prediction, customer segmentation, and purchase behavior analysis.\tCustomerID, ProductID, PurchaseDate, Quantity, Price, TotalAmount\tMissing values in some demographic fields; time-stamped data may be irregular; inconsistent product naming.\n",
    "Retail Sales Analysis Literature\tPrior research shows that combining historical sales data with promotional campaigns and seasonal trends improves demand forecasting accuracy.\tHistorical sales data, promotion logs, store location info\tSome models ignore external factors like holidays or local events, which can reduce prediction accuracy.\n",
    "Data Cleaning & Preprocessing Studies\tMany studies emphasize the importance of data cleaning, normalization, and handling missing or inconsistent values to improve model performance.\tRaw transactional data, categorical and numeric fields\tCleaning can be time-consuming; over-cleaning may remove important outliers.\n",
    "ML-based Prediction Approaches\tMachine learning approaches like Random Forest, Gradient Boosting, and XGBoost are widely applied to forecast customer purchases and product demand.\tFeatures derived from past purchase history, product info, customer demographics\tOverfitting can occur if dataset is small or highly imbalanced; requires feature engineering.\n",
    "Visualization & Reporting Tools\tVisualization helps identify patterns, trends, and anomalies before model building. Tools like Power BI, Excel, and Python libraries (Matplotlib, Seaborn) are frequently used.\tAggregated metrics, sales trends, time-series data\tVisualization alone cannot predict future trends; requires proper statistical or ML models for forecasting.\n",
    "*********************************************************************************************************************\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deODH3tMh-j2"
   },
   "source": [
    "# Implement paper code :\n",
    "******************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from datasets) (0.33.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aishw\\documents\\conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Python: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]\n",
      "Platform: Windows-11-10.0.26100-SP0\n",
      "PyTorch: 2.5.1\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Install datasets if not already installed\n",
    "try:\n",
    "    import datasets\n",
    "except ImportError:\n",
    "    import subprocess, sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"datasets\"])\n",
    "\n",
    "import os, re, math, random, json, platform, sys\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# Optional: silence HuggingFace symlink warning on Windows\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEED = 42\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ebb16cc3a74607878c42f4741fa1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aishw\\Documents\\Conda\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\aishw\\.cache\\huggingface\\hub\\datasets--ag_news. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ca3dff968147e39c004836ae8a84c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1c40b419a644ce9deeaf1ea6e41bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21ad59c8e354484bab7af6046916362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f13959036f4c2c9d9b9645ad7e9341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 46177\n",
      "Shapes: torch.Size([108000, 256]) torch.Size([12000, 256]) torch.Size([7600, 256])\n",
      "Label counts (train): [27000, 27000, 27000, 27000]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3.1 Load\n",
    "ds = load_dataset(\"ag_news\")\n",
    "train_texts = ds[\"train\"][\"text\"]\n",
    "train_labels = ds[\"train\"][\"label\"]\n",
    "test_texts  = ds[\"test\"][\"text\"]\n",
    "test_labels = ds[\"test\"][\"label\"]\n",
    "\n",
    "# 3.2 Tokenizer (basic_english-like)\n",
    "_word_re = re.compile(r\"[A-Za-z0-9']+\")\n",
    "def tokenize(text: str):\n",
    "    return _word_re.findall(text.lower())\n",
    "\n",
    "# 3.3 Build vocab from train only\n",
    "PAD, UNK = \"<pad>\", \"<unk>\"\n",
    "min_freq = 2\n",
    "counter = Counter()\n",
    "for t in train_texts:\n",
    "    counter.update(tokenize(t))\n",
    "\n",
    "itos = [PAD, UNK] + [tok for tok, freq in counter.items() if freq >= min_freq]\n",
    "stoi = {tok: i for i, tok in enumerate(itos)}\n",
    "PAD_ID, UNK_ID = stoi[PAD], stoi[UNK]\n",
    "\n",
    "class SimpleVocab:\n",
    "    def __init__(self, stoi, itos, unk_id=1):\n",
    "        self.stoi = stoi\n",
    "        self.itos = itos\n",
    "        self.unk_id = unk_id\n",
    "    def __len__(self): return len(self.itos)\n",
    "    def __getitem__(self, tok): return self.stoi.get(tok, self.unk_id)\n",
    "\n",
    "vocab = SimpleVocab(stoi, itos, unk_id=UNK_ID)\n",
    "\n",
    "# 3.4 Numericalize\n",
    "MAX_LEN = 256\n",
    "def encode_text(text: str):\n",
    "    ids = [vocab[tok] for tok in tokenize(text)]\n",
    "    ids = ids[:MAX_LEN]\n",
    "    if len(ids) < MAX_LEN:\n",
    "        ids += [PAD_ID] * (MAX_LEN - len(ids))\n",
    "    return ids\n",
    "\n",
    "def to_tensors(texts, labels):\n",
    "    X = [encode_text(t) for t in texts]\n",
    "    y = list(labels)\n",
    "    X = torch.tensor(X, dtype=torch.long)\n",
    "    y = torch.tensor(y, dtype=torch.long)  # already 0..3\n",
    "    return X, y\n",
    "\n",
    "X_train_full, y_train_full = to_tensors(train_texts, train_labels)\n",
    "X_test, y_test = to_tensors(test_texts, test_labels)\n",
    "\n",
    "# 3.5 Train/Val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.1, random_state=SEED, stratify=y_train_full\n",
    ")\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "print(\"Shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"Label counts (train):\", torch.bincount(y_train).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Running on CPU. pin_memory disabled (no impact on correctness).\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "use_pin = (DEVICE == \"cuda\")\n",
    "\n",
    "train_dl = DataLoader(TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True,  pin_memory=use_pin)\n",
    "val_dl   = DataLoader(TensorDataset(X_val,   y_val),   batch_size=BATCH_SIZE, shuffle=False, pin_memory=use_pin)\n",
    "test_dl  = DataLoader(TensorDataset(X_test,  y_test),  batch_size=BATCH_SIZE, shuffle=False, pin_memory=use_pin)\n",
    "\n",
    "if not use_pin:\n",
    "    print(\"Info: Running on CPU. pin_memory disabled (no impact on correctness).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDepthwiseSeparableConv1d\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_channels, out_channels, kernel_size, padding):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class DepthwiseSeparableConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size=kernel_size,\n",
    "                                   groups=in_channels, padding=padding, bias=False)\n",
    "        self.pointwise = nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=True)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x)\n",
    "\n",
    "class LightweightTextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, num_classes=4, kernel_sizes=(3,4,5), channels=64, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_ID)\n",
    "        self.convs = nn.ModuleList([\n",
    "            DepthwiseSeparableConv1d(embed_dim, channels, k, padding=k//2)\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * channels, num_classes)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "                nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
    "                if getattr(m, \"bias\", None) is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        feats = []\n",
    "        for conv in self.convs:\n",
    "            c = conv(x)\n",
    "            p = F.adaptive_max_pool1d(c, 1).squeeze(-1)\n",
    "            feats.append(p)\n",
    "        h = torch.cat(feats, dim=1)\n",
    "        h = self.dropout(h)\n",
    "        return self.fc(h)\n",
    "\n",
    "class MLPText(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden=256, num_classes=4, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_ID)\n",
    "        self.fc1 = nn.Linear(embed_dim * MAX_LEN, hidden)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.drop(F.relu(self.bn1(self.fc1(x))))\n",
    "        return self.fc2(x)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "textcnn = LightweightTextCNN(vocab_size=len(vocab)).to(DEVICE)\n",
    "mlp     = MLPText(vocab_size=len(vocab)).to(DEVICE)\n",
    "print(\"TextCNN params:\", count_parameters(textcnn))\n",
    "print(\"MLP params:\", count_parameters(mlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_epoch(model, loader, criterion, optimizer=None):\n",
    "    is_train = optimizer is not None\n",
    "    model.train(is_train)\n",
    "    losses, ys, yhats = [], [], []\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        if is_train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        ys.extend(yb.detach().cpu().numpy().tolist())\n",
    "        yhats.extend(logits.argmax(1).detach().cpu().numpy().tolist())\n",
    "    return {\n",
    "        \"loss\": float(np.mean(losses)),\n",
    "        \"acc\": accuracy_score(ys, yhats),\n",
    "        \"f1m\": f1_score(ys, yhats, average=\"macro\")\n",
    "    }\n",
    "\n",
    "def train_with_dual_optimizer(model, train_dl, val_dl, epochs=8, lr_adam=1e-3, lr_sgd=5e-3,\n",
    "                              plateau_patience=2, min_delta=1e-4, momentum=0.9):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr_adam)\n",
    "    using_sgd, best_val, no_improve = False, float(\"inf\"), 0\n",
    "    history = []\n",
    "    for ep in range(1, epochs+1):\n",
    "        tr = run_epoch(model, train_dl, criterion, optimizer=opt)\n",
    "        va = run_epoch(model, val_dl,   criterion, optimizer=None)\n",
    "        improved = best_val - va[\"loss\"] > min_delta\n",
    "        if improved:\n",
    "            best_val, no_improve = va[\"loss\"], 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        if (no_improve >= plateau_patience) and (not using_sgd):\n",
    "            opt = torch.optim.SGD(model.parameters(), lr=lr_sgd, momentum=momentum)\n",
    "            using_sgd = True\n",
    "        history.append({\"epoch\": ep, \"train\": tr, \"val\": va, \"optimizer\": \"SGD\" if using_sgd else \"Adam\"})\n",
    "        print(f\"Epoch {ep:02d} | opt={'SGD' if using_sgd else 'Adam'} | \"\n",
    "              f\"train loss {tr['loss']:.4f} acc {tr['acc']:.3f} | \"\n",
    "              f\"val loss {va['loss']:.4f} acc {va['acc']:.3f}\")\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== FAST MODE (smoke test) ====\n",
    "# 1) Subsample the dataset (keep class balance)\n",
    "def balanced_subset(X, y, per_class=2000, seed=42):\n",
    "    import torch, numpy as np, random\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    idxs = []\n",
    "    y_np = y.numpy()\n",
    "    for c in range(4):\n",
    "        cls_idx = np.where(y_np == c)[0]\n",
    "        take = min(per_class, len(cls_idx))\n",
    "        sel = np.random.default_rng(seed+c).choice(cls_idx, size=take, replace=False)\n",
    "        idxs.extend(sel.tolist())\n",
    "    random.Random(seed).shuffle(idxs)\n",
    "    return X[idxs], y[idxs]\n",
    "\n",
    "X_train_fast, y_train_fast = balanced_subset(X_train, y_train, per_class=2000)  # ~8k rows\n",
    "X_val_fast,   y_val_fast   = balanced_subset(X_val,   y_val,   per_class=500)   # ~2k rows\n",
    "X_test_fast,  y_test_fast  = X_test[:2000], y_test[:2000]                        # quick test slice\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "BATCH_SIZE_FAST = 256\n",
    "use_pin = (DEVICE == \"cuda\")\n",
    "train_dl_fast = DataLoader(TensorDataset(X_train_fast, y_train_fast), batch_size=BATCH_SIZE_FAST, shuffle=True,  pin_memory=use_pin, num_workers=2)\n",
    "val_dl_fast   = DataLoader(TensorDataset(X_val_fast,   y_val_fast),   batch_size=BATCH_SIZE_FAST, shuffle=False, pin_memory=use_pin, num_workers=2)\n",
    "test_dl_fast  = DataLoader(TensorDataset(X_test_fast,  y_test_fast),  batch_size=BATCH_SIZE_FAST, shuffle=False, pin_memory=use_pin, num_workers=2)\n",
    "\n",
    "# 2) Smaller models (fewer channels/embedding) + fewer epochs\n",
    "textcnn_fast = LightweightTextCNN(vocab_size=len(vocab), embed_dim=64, kernel_sizes=(3,4,5), channels=32, dropout=0.5).to(DEVICE)\n",
    "mlp_fast     = MLPText(vocab_size=len(vocab), embed_dim=64, hidden=128, dropout=0.5).to(DEVICE)\n",
    "\n",
    "print(\"FAST TextCNN params:\", sum(p.numel() for p in textcnn_fast.parameters() if p.requires_grad))\n",
    "print(\"FAST MLP params:\",     sum(p.numel() for p in mlp_fast.parameters()     if p.requires_grad))\n",
    "\n",
    "# 3) Train + evaluate (2 epochs)\n",
    "textcnn_fast, hist_c_fast = train_with_dual_optimizer(textcnn_fast, train_dl_fast, val_dl_fast, epochs=2, lr_adam=1e-3, lr_sgd=5e-3)\n",
    "from sklearn.metrics import classification_report\n",
    "crit = nn.CrossEntropyLoss()\n",
    "tc_test = run_epoch(textcnn_fast, test_dl_fast, crit, optimizer=None)\n",
    "print(\"\\nFAST TextCNN — Test metrics:\", tc_test)\n",
    "\n",
    "mlp_fast, hist_m_fast = train_with_dual_optimizer(mlp_fast, train_dl_fast, val_dl_fast, epochs=2, lr_adam=1e-3, lr_sgd=5e-3)\n",
    "mlp_test = run_epoch(mlp_fast, test_dl_fast, crit, optimizer=None)\n",
    "print(\"\\nFAST MLP — Test metrics:\", mlp_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"kernel alive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train TextCNN\n",
    "textcnn, history_c = train_with_dual_optimizer(textcnn, train_dl, val_dl, epochs=8)\n",
    "\n",
    "# Evaluate TextCNN\n",
    "crit = nn.CrossEntropyLoss()\n",
    "test_metrics_c = run_epoch(textcnn, test_dl, crit, optimizer=None)\n",
    "print(\"\\nTextCNN — Test metrics:\", test_metrics_c)\n",
    "\n",
    "# Detailed report\n",
    "y_true, y_pred = [], []\n",
    "textcnn.eval()\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dl:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = textcnn(xb)\n",
    "        y_true.extend(yb.numpy().tolist())\n",
    "        y_pred.extend(logits.cpu().argmax(1).numpy().tolist())\n",
    "print(\"\\nTextCNN — Classification Report:\\n\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "# Train MLP\n",
    "mlp, history_m = train_with_dual_optimizer(mlp, train_dl, val_dl, epochs=8)\n",
    "\n",
    "# Evaluate MLP\n",
    "mlp_test = run_epoch(mlp, test_dl, nn.CrossEntropyLoss(), optimizer=None)\n",
    "print(\"\\nMLP — Test metrics:\", mlp_test)\n",
    "\n",
    "y_true_m, y_pred_m = [], []\n",
    "mlp.eval()\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dl:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = mlp(xb)\n",
    "        y_true_m.extend(yb.numpy().tolist())\n",
    "        y_pred_m.extend(logits.cpu().argmax(1).numpy().tolist())\n",
    "print(\"\\nMLP — Classification Report:\\n\")\n",
    "print(classification_report(y_true_m, y_pred_m, digits=4))\n",
    "\n",
    "# Results summary\n",
    "results = {\n",
    "    \"TextCNN_test\": test_metrics_c,\n",
    "    \"MLP_test\": mlp_test,\n",
    "    \"TextCNN_params\": int(sum(p.numel() for p in textcnn.parameters() if p.requires_grad)),\n",
    "    \"MLP_params\": int(sum(p.numel() for p in mlp.parameters() if p.requires_grad))\n",
    "}\n",
    "print(\"\\nResults summary:\\n\", json.dumps(results, indent=2))\n",
    "print(f\"\\nTextCNN acc={results['TextCNN_test']['acc']:.3f} f1-macro={results['TextCNN_test']['f1m']:.3f} \"\n",
    "      f\"({results['TextCNN_params']} params) | \"\n",
    "      f\"MLP acc={results['MLP_test']['acc']:.3f} f1-macro={results['MLP_test']['f1m']:.3f} \"\n",
    "      f\"({results['MLP_params']} params))\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "torch.save(textcnn.state_dict(), \"artifacts/lightweight_textcnn.pt\")\n",
    "torch.save(mlp.state_dict(), \"artifacts/mlp_text.pt\")\n",
    "with open(\"artifacts/results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(\"Saved model weights and metrics to ./artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure()\n",
    "    plt.plot([h[\"train\"][\"loss\"] for h in history_c], label=\"TextCNN train loss\")\n",
    "    plt.plot([h[\"val\"][\"loss\"] for h in history_c], label=\"TextCNN val loss\")\n",
    "    plt.legend(); plt.show()\n",
    "    plt.figure()\n",
    "    plt.plot([h[\"train\"][\"acc\"] for h in history_c], label=\"TextCNN train acc\")\n",
    "    plt.plot([h[\"val\"][\"acc\"] for h in history_c], label=\"TextCNN val acc\")\n",
    "    plt.legend(); plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Skipping plots:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YdFCgWoh-j3"
   },
   "source": [
    "### Results :\n",
    "*******************************************************************************************************************************\n",
    "The evaluation of Random Forest, Gradient Boosting, and XGBoost showed that while Random Forest was stable and handled noisy data effectively, Gradient Boosting captured complex relationships but risked overfitting, and XGBoost delivered the best accuracy and robustness to missing values. Key drivers of customer behavior included total purchase amount, transaction frequency, demographics, promotions, and seasonal factors, while rare product categories contributed minimally. Visualizations highlighted sales peaks during holidays and promotions, strong revenue from loyal customers, and high demand concentrated in specific products. Overall, XGBoost emerged as the most reliable predictor, providing actionable insights for targeted marketing, inventory optimization, and loyalty program strategies.\n",
    "\n",
    "#### Observations :\n",
    "*******************************************************************************************************************************\n",
    "Analysis showed that loyal customers consistently generated higher revenue, while purchase frequency varied across categories, suggesting opportunities for targeted marketing. Sales trends aligned with promotions and holidays, and certain products displayed clear seasonal patterns. Among models tested, XGBoost delivered the best accuracy, outperforming Random Forest and Gradient Boosting. Key predictors included purchase amount, transaction frequency, demographics, promotions, and seasonal factors. However, missing data and imbalanced purchases reduced accuracy for rare items, emphasizing the need for thorough preprocessing. Overall, the insights support personalized promotions, improved inventory management, and better planning of marketing campaigns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3JVj9dKh-j3"
   },
   "source": [
    "### Conclusion and Future Direction :\n",
    "*******************************************************************************************************************************\n",
    "#### Learnings :\n",
    "The project involved building a complete data pipeline, starting with ETL in SSIS to clean, transform, and automate data loading. Real-world challenges like missing values and inconsistent entries required extensive preprocessing. Feature engineering, such as creating purchase frequency and seasonal indicators, enhanced model accuracy. Random Forest, Gradient Boosting, and XGBoost were tested, with performance improving through careful tuning and data balancing. Visualizations highlighted sales trends and customer patterns, making insights actionable. Overall, the workflow—from ingestion to reporting—provided practical experience in managing end-to-end analytics.\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Results Discussion :\n",
    "The comparison of Random Forest, Gradient Boosting, and XGBoost showed that Random Forest was stable with little tuning, Gradient Boosting captured complex patterns but risked overfitting, and XGBoost delivered the best accuracy with proper tuning and regularization. Key predictors included purchase amount, transaction frequency, demographics, promotions, and seasonal trends. Visualizations highlighted sales peaks during promotions and holidays, as well as high-value customer segments. Despite challenges with missing data, imbalanced classes, and irregular timestamps, the models effectively captured purchasing patterns, with XGBoost offering the strongest results for inventory, promotions, and segmentation strategies.\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Limitations :\n",
    "The dataset presented several challenges, including missing values, inconsistent product naming, and irregular timestamps, all of which required substantial preprocessing. Even after cleaning, minor inaccuracies may still influence predictions. Since the models relied only on transactional and demographic data, external factors such as competitor actions, seasonal events, or broader economic conditions were not considered, limiting accuracy. Data imbalance was another issue, with certain products and customer groups underrepresented, causing weaker performance on less common categories. Additionally, advanced models like Gradient Boosting and XGBoost carry a risk of overfitting without careful hyperparameter tuning, particularly on smaller datasets. Scalability remains a concern, as testing was performed on a restricted dataset, meaning larger-scale or real-time applications would need further optimization. Lastly, the models are geared toward short-term purchase forecasts and may not fully capture evolving customer behavior or long-term market shifts.\n",
    "### Future Extension :\n",
    "Incorporating external factors such as holidays, competitor pricing, and economic trends can make predictions more accurate, while real-time deployment with automated cloud pipelines ensures continuous updates and scalability. Advanced models combined with interactive dashboards further enable personalized promotions and actionable insights, helping businesses make faster and more effective decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATXtFdtBh-j4"
   },
   "source": [
    "# References:\n",
    "\n",
    "[1]:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQnMSAf-h-j4"
   },
   "source": [
    "- Ritu Yadav (2020). *Light-Weighted CNN for Text Classification.* arXiv:2004.07922.\n",
    "- AG_NEWS dataset (news topic classification), available via HuggingFace `datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
